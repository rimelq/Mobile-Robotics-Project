





























#### CONSTANTS
# constants for movement
DIST_TARGET_CTRL = 30  # min distance between the robot and a target that counts as "the robot cleared the target"
SPEED = 150

# constants for taking into account the distance between the plan and the camera
SCALE_IMG = 55
SCALING_FACTOR = 40

### initialization of the robot 
thymio = Robot(DIST_TARGET_CTRL,SPEED)

### initialization of the computer vision
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  
[template_rbt, template_goal] = defTemplate(SCALE_IMG)
goal = [0, 0]
foundG = False

#### initialization of the FSM
state = 0
is_finished = False

# check if the camera is open
if not cap.isOpened():
    print("Error: Could not access the camera")
else:
    # allow some time for the camera to initialize
    time.sleep(2)
    ### MAIN LOOP
    while True: 
        success, image = cap.read()
        # check if the camera reads images
        if not success:
            cap.release()
            print("Error: Could not read the camera")
            break
        
        
        clear_output(True)
        
        ## initial situation, localisation of the rbt, the goal and the obstacles
        ## once everything is located press "l" to start the program 
        if state == 0:
            [goal, obst_list] = globalPlanning(image, thymio, SCALING_FACTOR, template_rbt, template_goal)
            if cv2.waitKey(1) & 0xFF==ord('l'): 
                state = 1
                
        ## robot moving
        # check if the robot is either kidnapped or meeting a local obstacles 
        if(state == 1 or state == 2):
            state = updateState(state, client, node, aw, thymio)

        # robot moving following the global navigation and leaving the loop if the goal has been reaching
        if state == 1:            
            is_finished = await robotMoving(thymio, image, template_rbt, node, client, SCALE_IMG)     
            if (is_finished):
                cap.release()
                break 
                
        # robot moving in case of local obstacle
        if state == 2:
            await localAvoidance(SPEED, node, aw, thymio, client, SCALE_IMG, state, image, template_rbt)

        # wait some time after kidnapping detection, then go back to state = 0 to re scan the entire map
        if state == 3:
            obst_list = []
            if((time.time() - thymio.kid_start) > 5):
                state = 0

        # after avoiding a local obstacle, go forward to clear the obstacle
        if state == 4:
            # Track the robot's position using vision
            [(thymio.x_cam, thymio.y_cam), thymio.found] = robotTracking(image, template_rbt)
            
            thymio.x, thymio.var_x = kalman(thymio.x_enc, thymio.x_cam, thymio.var_x, VAR_CAM, VAR_ENC, thymio.found)
            thymio.y, thymio.var_y = kalman(thymio.y_enc, thymio.y_cam, thymio.var_y, VAR_CAM, VAR_ENC, thymio.found)
            thymio.x_enc, thymio.y_enc = thymio.x, thymio.y

            # time variable for the computation of the encoder position
            thymio.last_move = time.time()
            thymio.start_move_old = thymio.start_move
            thymio.start_move = time.time()
            thymio.x_enc, thymio.y_enc = await thymio.pos_encodeur(node, client, SCALE_IMG)
            
            if((time.time() - thymio.avoid_time) > 2):
                state = 1      
        
 
        # image drawing and display
        drawRobot(image, thymio, template_rbt, False)
        drawGoal(image, goal, template_goal)
        drawObstacle(image, obst_list)
        cv2.imshow('Computer Vision', image) 
        
        # quit when 'q' is pressed
        if cv2.waitKey(1) & 0xFF==ord('q'): 
            cap.release()
            break

stop_robot(node)
cv2.destroyAllWindows()




















# find obstacle using range and thresholding
def findObstacle(image):
    # define the range of value that are considered as black color
    lower_range = np.array([0, 0, 0], dtype = "uint8")
    upper_range = np.array([UP_VAL, UP_VAL, UP_VAL], dtype = "uint8")
    mask = cv2.inRange(image, lower_range, upper_range)
    # filtering
    filtered_img = cv2.medianBlur(mask, KERNEL) 
    # thresholding
    ret, image_obst = cv2.threshold(filtered_img, OBST_THRESHOLD, 255, 0)  
    return image_obst





# find goal using matchTemplate
def findGoal(image, template):
    w, h  = template.shape[::-1]
    # change to grayscale in order to use matchTemplate
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    res = cv2.matchTemplate(image_gray, template, cv2.TM_CCOEFF_NORMED)
    _, max_val, _, max_loc = cv2.minMaxLoc(res)
    # test if the matching is high enough
    if max_val > GOAL_THRESHOLD:
        found = True
        # get the middle of the template
        center = (max_loc[0] + w//2, max_loc[1] + h//2)
        return [center, found]
    else: 
        found = False
        return [[], found]





# track the robot using matchTemplate
def robotTracking(image, template):
    w, h  = template.shape[::-1]
    # change to grayscale in order to use matchTemplate
    image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    res = cv2.matchTemplate(image_gray, template, cv2.TM_CCOEFF_NORMED)
    _, max_val, _, max_loc = cv2.minMaxLoc(res)
    # test if the matching is high enough
    if max_val > ROBOT_THRESHOLD:
        found = True
        # get the middle of the template
        center = (max_loc[0] + w//2, max_loc[1] + h//2)
        return [center, found]
    else: 
        found = False
        return [(0, 0), found]














# Simplifies the path using the Ramer-Douglas-Peucker algorithm to have at most max_points.
def simplify_path_rdp(path, max_points=20):
    # Ramer-Douglas-Peucker algorithm.
    def rdp(points, epsilon):
        if len(points) < 3:
            return points

        # Line from start to end
        start, end = points[0], points[-1]
        line = end - start
        line_norm = np.hypot(line[0], line[1])
        if line_norm == 0:
            dists = np.hypot(*(points - start).T)
        else:
            dists = np.abs(np.cross(line, start - points)) / line_norm

        index = np.argmax(dists)
        max_dist = dists[index]

        if max_dist > epsilon:
            # Recursive call
            left = rdp(points[:index+1], epsilon)
            right = rdp(points[index:], epsilon)
            return np.vstack((left[:-1], right))
        else:
            return np.array([start, end])

    # Convert path to NumPy array
    points = np.array(path)

    # Adjust epsilon to get at most max_points
    epsilon_min = 0
    epsilon_max = np.hypot(*(points[-1] - points[0]))  # Maximum possible distance

    # Binary search to find the right epsilon
    for _ in range(100):
        epsilon = (epsilon_min + epsilon_max) / 2
        simplified = rdp(points, epsilon)
        if len(simplified) > max_points:
            epsilon_min = epsilon
        else:
            epsilon_max = epsilon
        if epsilon_max - epsilon_min < 1e-5:
            break

    # Final simplified path
    simplified = rdp(points, epsilon_max)
    return simplified.astype(int)








!pip install opencv-python --quiet
import cv2
from globalNav import show_image, find_shortest_path

# Load the image
image_path = "./img/image_obst.png"
image = cv2.imread(image_path)
scalingFactor = 30
start = (56, 56)  # Start coordinates
goal = (600, 600)  # Goal coordinates
verbose = True  # Enable visualizations

shortest_path, contours = find_shortest_path(image, scalingFactor, start, goal, verbose)

print("Shortest Path:\n", shortest_path)











# check for local obstacle or kidnapping
def updateState(state, client, node, aw, thymio):
    # Read proximity sensor values
    prox_horizontal = read_prox_sensors(node, client, aw)
    obst = [prox_horizontal[i] for i in range(5)]  # Sensors 0 to 4

    # State 1: Goal Tracking
    if state == 1:
        # Check if any sensor detects an obstacle
        if any(obst[i] > OBST_THRESHOLD_HIGH for i in range(5)):
            state = 2  # Switch to Obstacle Avoidance

    # State 2: Obstacle Avoidance
    elif state == 2:
        # Check if all sensors are below the low threshold
        if all(obst[i] < OBST_THRESHOLD_LOW for i in range(5)):
            state = 4  # Switch back to Goal Tracking
            thymio.avoid_time = time.time()
            node.send_set_variables(motors(thymio.SPEED, thymio.SPEED))

    # Check for kidnapping
    if kidnappingDetection(node, client):
        stop_robot(node)
        state = 3  # Kidnapped state
        thymio.kid_start = time.time()

    return state





# Smooths the transition between two speeds.
def smooth_speed(previous_speed, new_speed, smoothing_factor=0.9):
    return smoothing_factor * previous_speed + (1 - smoothing_factor) * new_speed
    
def local_nav(sensitivity_factor, smoothing_factor, prox_horizontal, desired_speed, previous_speed):
    """
    Calculates motor speeds for obstacle avoidance using an ANN approach.
    """
    # weights for the neural network (left and right motors)
    weights_left = [-40, -20, -20, 20, 35, -10, 30, 0, 8]
    weights_right = [40, 20, -20, -20, -35, 30, -10, 8, 0]

    # scaling factors
    SENSOR_SCALE = 800
    MEMORY_SCALE = 20

    # initialize neural network inputs
    nn_inputs = [0] * 9

    # memory inputs (scaled previous speeds)
    nn_inputs[7] = desired_speed[0] / MEMORY_SCALE
    nn_inputs[8] = desired_speed[1] / MEMORY_SCALE

    # process proximity sensor inputs with sensitivity adjustment
    for i in range(7):
        nn_inputs[i] = (prox_horizontal[i] / SENSOR_SCALE) * sensitivity_factor

    # compute motor outputs from ANN neurons
    motor_speeds = [0, 0]
    for i in range(len(nn_inputs)):
        motor_speeds[0] += nn_inputs[i] * weights_left[i]
        motor_speeds[1] += nn_inputs[i] * weights_right[i]

    # smooth the motor speeds to avoid abrupt changes
    motor_speeds[0] = int(smooth_speed(previous_speed[0], motor_speeds[0], smoothing_factor))
    motor_speeds[1] = int(smooth_speed(previous_speed[1], motor_speeds[1], smoothing_factor))

    return motor_speeds

















class Robot:
    def __init__(self,dist_target,speed):
        self.DIST_TARGET_CTRL = dist_target
        self.SPEED = speed
        # position variables
        self.x = 0  
        self.y = 0
        self.x_old = 0
        self.y_old = 0
        # encoder
        self.x_enc = 0  
        self.y_enc = 0
        self.x_enc_old = 0
        self.y_enc_old = 0
        # camera
        self.x_cam = 0  
        self.y_cam = 0
        # variance of the estimate position of the thymio 
        self.var_x = 0
        self.var_y = 0
        
        self.old_err = 0

        # list of the points that the robot needs to go to
        self.targets = []
        self.target_Id = 1
        self.found = False
        
        self.last_move = 0
        self.start_move_old = 0
        self.start_move  = 0

        # starting time of the kidnapping
        self.kid_start = 0
        # starting time of the local obstacle avoidance
        self.avoid_time = 0








## Method of the robot class
# convert cartesian coordinates to thymio relative coordinates in order to get the direction of the current target
def thymio_base(self):
    B = np.array([self.x_old, self.y_old])   
    A = np.array([self.x, self.y])  
    C = np.array(self.targets[self.target_Id]) 
    
    AB = A - B

    v2 = AB / np.linalg.norm(AB) 
    v1 = np.array([-v2[1], v2[0]]) 
    C_prime = C - A
    new_C = np.array([np.dot(C_prime, v1), np.dot(C_prime, v2)])
    return new_C

# proportionnal integrator controler to
def pi(self, err):
    self.old_err =  err
    integrale = 0
    if abs(integrale) <= MAX_INT:
        integrale += err
        
    u = KP * err + KI * integrale 
    return u

# compute the command to give to the motor in order to reach the current target
def control(self):
    target_bar = self.thymio_base()
    if target_bar[0] == 0:
        err = 0
    else:
        err = abs(math.atan(target_bar[0]/target_bar[1])) * target_bar[0] / abs(target_bar[0])
    diff_speed = self.pi(err)  
    return diff_speed

# give command to the robot to make it go to its current target
def move_robot(self, node):
    # check if the robot has move sufficiently to get a new command
    if (eucl_distance(self.x, self.y, self.x_old, self.y_old) > MIN_DIST):
        dist_from_target = eucl_distance(self.targets[self.target_Id][0], self.targets[self.target_Id][1], self.x, self.y)

        # check if the robot is close enough to its target
        if dist_from_target <= self.DIST_TARGET_CTRL: 
            self.target_Id  += 1
            if(self.target_Id == len(self.targets)): # return True is last target reached
                return True

        # get the command 
        direction = self.control()
        node.send_set_variables(motors(self.SPEED + int(direction) , self.SPEED - int(direction)))

        # time variable for the computation of the encoder position
        self.last_move = time.time()
        self.start_move_old = self.start_move
        self.start_move = time.time()

        # new coordinates
        self.x_old = self.x
        self.y_old = self.y
    return False


async def pos_encodeur(self, node, client, scale):
    delta_time = self.last_move - self.start_move_old
    if delta_time > MAX_DELTA_TIME:
        delta_time = AVERAGE_LOOP_TIME
    x,y = self.x_enc,self.y_enc
    x_old,y_old = self.x_enc_old, self.y_enc_old
    self.x_enc_old, self.y_enc_old = self.x_enc, self.y_enc
    delta_x, delta_y, delta_angle = 0, 0, 0
    if  (x_old == x) and (y_old == y):
        print("X_old = x et y_old = y pour les encodeurs dans pos_encodeur")
        return x + 1, y + 1
    left_speed, right_speed = await get_speed(node,client)                                 # get the speed
    delta_x, delta_y,delta_angle = convert(left_speed, right_speed, delta_time, scale)        # get the delta speed
    pos_x, pos_y = to_cartesian_base(x, y, x_old, y_old, delta_x, delta_y)
    return pos_x, pos_y





## Functions in the control module
# set the motors speed to the desired one
def motors(left, right):
    return {
        "motor.left.target": [left],
        "motor.right.target": [right],
    }

# compute the enclidian distance between two points
def eucl_distance(x1, y1, x2, y2):
    return ((x1-x2)**2 + (y1-y2)**2)**0.5

def stop_robot(node):
    node.send_set_variables(motors(0, 0))

# get the speed of the robot using the encoders
async def get_speed(node,client):
    await node.wait_for_variables({"motor.left.speed", "motor.right.speed"})
    await client.sleep(TIME_TO_GET)
    left_speed, right_speed = node.v.motor.left.speed, node.v.motor.right.speed
    return left_speed, right_speed 

# convert the speed from the encoder and compute the relative variation of position
def convert(left_speed, right_speed,time_s,scale):
    angular_speed_cst = SCALED_ANGULAR_SPEED * scale
    angular_speed = (right_speed - left_speed) * angular_speed_cst
    delta_angle = angular_speed * time_s                              # angle in radians
    delta_x =  SCALED_SPEED * scale * np.cos(delta_angle) * time_s         
    delta_y = SCALED_SPEED * scale * np.sin(delta_angle) * time_s  
    return delta_x, delta_y,delta_angle

# convert relative coordinates of the thymio to absolute cartesian coordinates
def to_cartesian_base(x, y, x_old, y_old, pos_x, pos_y):
    B = np.array([x_old, y_old])   
    A = np.array([x, y])
    C = np.array([pos_x, pos_y])
    AB = A - B
    v2 = AB / np.linalg.norm(AB)
    v1 = np.array([v2[1], -v2[0]])
    abs_pos = np.array([np.dot(C, v2), np.dot(C, v1)])
    real_val = np.add(abs_pos, A)
    return real_val[0], real_val[1]























## Code for the Kalman filter
def kalman(pos_enc, pos_cam, pos_var, var_cam, var_enc, found):
    if found:
        sum_var = pos_var + var_enc
        i_t = pos_cam - pos_enc
        k_t = sum_var/(sum_var + var_cam)
        new_pos = pos_enc + k_t * i_t
        new_var = (1 - k_t)*sum_var
    else:
        new_pos = pos_enc
        new_var = pos_var + var_enc
    return new_pos, new_var











# kidnapping detection using ground proximity sensor, if the robot is lifted then it is kidnapped
# ground proximity sensor threshold for kidnapping
PROX_THRESHOLD = 50

def kidnappingDetection(node, client):
    aw(node.wait_for_variables({"prox.ground.delta"}))
    aw(client.sleep(0.01))
    prox = node.v.prox.ground.delta[1]
    if (prox < PROX_THRESHOLD):
        return True
    else:
        return False





























### import 
from IPython.display import display, Image
import ipywidgets as widgets
from IPython.display import clear_output
from FSM import *


### initiate communication with the Thymio
client = ClientAsync()
node = await client.wait_for_node()
await node.lock()


### Main loop

#### CONSTANTS
# constants for movement
DIST_TARGET_CTRL = 30  # min distance between the robot and a target that counts as "the robot cleared the target"
SPEED = 150

# constants for taking into account the distance between the plan and the camera
SCALE_IMG = 55
SCALING_FACTOR = 40

### initialization of the robot 
thymio = Robot(DIST_TARGET_CTRL,SPEED)

### initialization of the computer vision
cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  
[template_rbt, template_goal] = defTemplate(SCALE_IMG)
goal = [0, 0]
foundG = False

#### initialization of the FSM
state = 0
is_finished = False

# check if the camera is open
if not cap.isOpened():
    print("Error: Could not access the camera")
else:
    # allow some time for the camera to initialize
    time.sleep(2)
    ### MAIN LOOP
    while True: 
        success, image = cap.read()
        # check if the camera reads images
        if not success:
            cap.release()
            print("Error: Could not read the camera")
            break
        
        
        clear_output(True)
        
        ## initial situation, localisation of the rbt, the goal and the obstacles
        ## once everything is located press "l" to start the program 
        if state == 0:
            [goal, obst_list] = globalPlanning(image, thymio, SCALING_FACTOR, template_rbt, template_goal)
            if cv2.waitKey(1) & 0xFF==ord('l'): 
                state = 1
                
        ## robot moving
        # check if the robot is either kidnapped or meeting a local obstacles 
        if(state == 1 or state == 2):
            state = updateState(state, client, node, aw, thymio)

        # robot moving following the global navigation and leaving the loop if the goal has been reaching
        if state == 1:            
            is_finished = await robotMoving(thymio, image, template_rbt, node, client, SCALE_IMG)     
            if (is_finished):
                cap.release()
                break 
                
        # robot moving in case of local obstacle
        if state == 2:
            await localAvoidance(SPEED, node, aw, thymio, client, SCALE_IMG, state, image, template_rbt)

        # wait some time after kidnapping detection, then go back to state = 0 to re scan the entire map
        if state == 3:
            obst_list = []
            if((time.time() - thymio.kid_start) > 5):
                state = 0

        # after avoiding a local obstacle, go forward to clear the obstacle
        if state == 4:
            # Track the robot's position using vision
            [(thymio.x_cam, thymio.y_cam), thymio.found] = robotTracking(image, template_rbt)
            
            thymio.x, thymio.var_x = kalman(thymio.x_enc, thymio.x_cam, thymio.var_x, VAR_CAM, VAR_ENC, thymio.found)
            thymio.y, thymio.var_y = kalman(thymio.y_enc, thymio.y_cam, thymio.var_y, VAR_CAM, VAR_ENC, thymio.found)
            thymio.x_enc, thymio.y_enc = thymio.x, thymio.y

            # time variable for the computation of the encoder position
            thymio.last_move = time.time()
            thymio.start_move_old = thymio.start_move
            thymio.start_move = time.time()
            thymio.x_enc, thymio.y_enc = await thymio.pos_encodeur(node, client, SCALE_IMG)
           
            if((time.time() - thymio.avoid_time) > 2):
                state = 1      
        
 
        # image drawing and display
        drawRobot(image, thymio, template_rbt, False)
        drawGoal(image, goal, template_goal)
        drawObstacle(image, obst_list)
        cv2.imshow('Computer Vision', image) 
        
        # quit when 'q' is pressed
        if cv2.waitKey(1) & 0xFF==ord('q'): 
            cap.release()
            break

stop_robot(node)
cv2.destroyAllWindows()
